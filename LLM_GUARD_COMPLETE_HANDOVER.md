# LLM Guard v1.0 - Complete Handover Document

**Date**: 2025-10-09 KST
**Version**: 1.0.0
**Status**: Policy complete, runtime implementation partial
**Policy Signature**: `a4dec83545592db3f3d7f3bdfaaf556a325e2c78f5ce7a39813ec6a077960ad2`

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [System Architecture](#system-architecture)
3. [Current Implementation](#current-implementation)
4. [Policy Specification](#policy-specification)
5. [Schemas](#schemas)
6. [Test Cases](#test-cases)
7. [Examples](#examples)
8. [Implementation Roadmap](#implementation-roadmap)
9. [File Locations](#file-locations)

---

## Executive Summary

### What is LLM Guard?

LLM Guard is a policy-based validation system that ensures LLM-generated responses:
- **Are grounded in evidence** (no hallucinations)
- **Match confidence levels** (no overconfident claims)
- **Respect scope boundaries** (no medical/legal/investment advice)
- **Protect privacy** (PII detection and masking)
- **Use Korean-first labels** (KO-first compliance)
- **Maintain traceability** (immutable trace metadata)

### Current State

- âœ… **Policy v1.0**: Complete (181 lines, 9 rules, signed)
- âœ… **Schemas**: Input/output validation (JSON Schema Draft 2020-12)
- âœ… **Test cases**: 18 JSONL scenarios covering all rules
- âœ… **Minimal runtime**: Basic wrapper (63 lines) - production ready
- ğŸŸ¡ **Full runtime**: Needs implementation (~500 lines estimated)

### Key Metrics

- **Rules**: 9 (3 deny, 6 revise/warn)
- **Modality ranges**: 3 (0.0-0.49, 0.5-0.79, 0.8-1.0)
- **PII patterns**: 4 (phone, email, address, SSN)
- **Exit codes**: allow, revise, deny
- **Risk score**: 0-100 (weighted by severity)

---

## System Architecture

### Execution Flow

```
LLM Response (candidate_answer)
    â†“
LLM Guard Input Validator
    â†“
Sequential Rule Evaluation (STRUCT â†’ EVID â†’ SCOPE â†’ MODAL â†’ REL â†’ SIG â†’ PII â†’ KO â†’ AMBIG)
    â†“
First Failure â†’ Immediate Action (deny/revise)
    â†“
All Pass â†’ allow
    â†“
LLM Guard Output (decision, reasons, remediations, risk_score, logs)
```

### Rule Evaluation Order

1. **STRUCT-000**: Schema validation (deny on fail)
2. **EVID-BIND-100**: Evidence binding check (revise on fail)
3. **SCOPE-200**: Out-of-scope requests (deny on fail)
4. **MODAL-300**: Modality matching (revise on fail)
5. **REL-400**: Relation consistency (revise on fail)
6. **SIG-500**: Policy signature verification (deny on fail)
7. **PII-600**: PII detection (revise/deny on fail)
8. **KO-700**: Korean label compliance (revise on fail)
9. **AMBIG-800**: Source citation clarity (revise on fail)

**Note**: First failure stops evaluation and returns action. All results logged in `logs.trace[]`.

---

## Current Implementation

### File: `services/analysis-service/app/core/llm_guard.py`

**Lines**: 63
**Status**: âœ… Production-ready (minimal version)

```python
"""LLM validation and guard utilities for analysis responses."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable

from ..models import AnalysisResponse
from .recommendation import RecommendationGuard
from .text_guard import TextGuard


@dataclass(slots=True)
class LLMGuard:
    """Provide JSON validation and post-processing for LLM workflow."""

    text_guard: TextGuard
    recommendation_guard: RecommendationGuard

    @classmethod
    def default(cls) -> "LLMGuard":
        return cls(
            text_guard=TextGuard.from_file(),
            recommendation_guard=RecommendationGuard.from_file(),
        )

    def prepare_payload(self, response: AnalysisResponse) -> dict:
        """Convert response to plain dict before giving it to an LLM."""
        # Pydantic ensures structure; raising early if invalid.
        AnalysisResponse.model_validate(response.model_dump())
        return response.model_dump()

    def postprocess(
        self,
        original: AnalysisResponse,
        llm_payload: dict,
        *,
        structure_primary: str | None = None,
        topic_tags: Iterable[str] | None = None,
    ) -> AnalysisResponse:
        """Validate LLM output and enforce guards."""
        candidate = AnalysisResponse.model_validate(llm_payload)

        # Trace must remain identical (LLMì€ ìˆ˜ì • ë¶ˆê°€).
        if candidate.trace != original.trace:
            raise ValueError("LLM output modified trace metadata")

        # Text guard for trace notes (í–¥í›„ í™•ì¥ ëŒ€ë¹„).
        notes = candidate.trace.get("notes")
        if isinstance(notes, str):
            candidate.trace["notes"] = self.text_guard.guard(notes, topic_tags or [])

        # Recommendation guard: resultëŠ” traceì— advisory í•„ë“œë¡œ ê¸°ë¡.
        rec = self.recommendation_guard.decide(structure_primary=structure_primary)
        candidate.trace.setdefault("recommendation", rec)
        candidate.recommendation = candidate.recommendation.__class__(**rec)

        return candidate

    def validate_llm_output(self, llm_payload: dict) -> None:
        """Standalone validation for test/CI usage."""
        AnalysisResponse.model_validate(llm_payload)
```

### Tests: `services/analysis-service/tests/test_llm_guard.py`

**Lines**: 35
**Status**: âœ… 2/2 passing

```python
from app.core.engine import AnalysisEngine
from app.core.llm_guard import LLMGuard
from app.models import AnalysisRequest


def test_llm_guard_roundtrip() -> None:
    engine = AnalysisEngine()
    guard = LLMGuard.default()
    request = AnalysisRequest(pillars={}, options={})
    response = engine.analyze(request)
    payload = guard.prepare_payload(response)
    result = guard.postprocess(
        response,
        payload,
        structure_primary=response.structure.primary,
        topic_tags=["ê±´ê°•"],
    )
    assert result.trace["recommendation"]["enabled"] is False
    assert result.recommendation.enabled is False


def test_llm_guard_detects_trace_mutation() -> None:
    engine = AnalysisEngine()
    guard = LLMGuard.default()
    request = AnalysisRequest(pillars={}, options={})
    response = engine.analyze(request)
    payload = guard.prepare_payload(response)
    payload["trace"]["rule_id"] = "mutated"
    try:
        guard.postprocess(response, payload, structure_primary=response.structure.primary)
    except ValueError:
        pass
    else:
        raise AssertionError("Expected ValueError when trace mutated")
```

---

## Policy Specification

### File: `policy/llm_guard_policy_v1.json`

**Lines**: 181
**Signature**: `a4dec83545592db3f3d7f3bdfaaf556a325e2c78f5ce7a39813ec6a077960ad2`
**Status**: âœ… Signed and verified

```json
{
  "engine": "llm_guard",
  "policy_version": "1.0.0",
  "policy_date": "2025-10-09",
  "policy_signature": "a4dec83545592db3f3d7f3bdfaaf556a325e2c78f5ce7a39813ec6a077960ad2",
  "ko_labels": true,
  "dependencies": [
    "strength_policy_v2.json",
    "relation_policy_v1.1.x",
    "evidence_builder_v2.json"
  ],
  "evaluation_order": [
    "STRUCT-000",
    "EVID-BIND-100",
    "SCOPE-200",
    "MODAL-300",
    "REL-400",
    "SIG-500",
    "PII-600",
    "KO-700",
    "AMBIG-800"
  ],
  "rules": [
    {
      "rule_id": "STRUCT-000",
      "severity": "error",
      "when": "ì…ë ¥ ìŠ¤í‚¤ë§ˆ ìœ„ë°˜ ë˜ëŠ” í•„ìˆ˜ í•„ë“œ ëˆ„ë½",
      "check_type": "schema",
      "action": "deny",
      "reason_code": "INPUT-INVALID",
      "message_ko": "ì…ë ¥ êµ¬ì¡°ê°€ ìŠ¤í‚¤ë§ˆë¥¼ ìœ„ë°˜í–ˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "ì…ë ¥ ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ì—¬ ì¬ìš”ì²­í•˜ì„¸ìš”",
      "evidence_refs": []
    },
    {
      "rule_id": "EVID-BIND-100",
      "severity": "error",
      "when": "ì‚¬ì‹¤ ì£¼ì¥ì— ëŒ€ì‘í•˜ëŠ” evidence_id ë°”ì¸ë”©ì´ ëˆ„ë½ë¨",
      "check_type": "match",
      "action": "revise",
      "reason_code": "LLM-CLAIM-NOEVID",
      "message_ko": "ê·¼ê±° ì—†ëŠ” ì‚¬ì‹¤ ì£¼ì¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "ëª¨ë“  ì‚¬ì‹¤ ì£¼ì¥ì€ evidence.sources[].evidence_idë¥¼ ì¸ìš©í•˜ì„¸ìš”",
      "evidence_refs": ["evidence.sources[].evidence_id"]
    },
    {
      "rule_id": "SCOPE-200",
      "severity": "error",
      "when": "ì˜ë£Œ/ë²•ë¥ /íˆ¬ì êµ¬ì²´ ë‹¨ì •, ì¶œìƒì‹œê° ì¶”ì •, ì‚¬ë§ì¼ ì˜ˆì¸¡ ë“± ë²”ìœ„ ì™¸ ìš”ì²­",
      "check_type": "match",
      "action": "deny",
      "reason_code": "OUT-OF-SCOPE",
      "message_ko": "í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë²”ìœ„ì˜ ìš”ì²­ì…ë‹ˆë‹¤",
      "remediation_hint_ko": "ì˜ë£Œ/ë²•ë¥ /íˆ¬ì ë‹¨ì •, ì¶œìƒì‹œê° ì¶”ì •, ì‚¬ë§ì¼ ì˜ˆì¸¡ì€ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
      "evidence_refs": []
    },
    {
      "rule_id": "MODAL-300",
      "severity": "warn",
      "when": "ê·¼ê±° confidence < 0.5ì¸ë° ë‹¨ì • í‘œí˜„('í™•ì‹¤í•˜ë‹¤', '~ì´ë‹¤') ì‚¬ìš©",
      "check_type": "calc",
      "action": "revise",
      "reason_code": "MODALITY-OVERCLAIM",
      "message_ko": "ê·¼ê±° ì‹ ë¢°ë„ì— ë¹„í•´ ê³¼ë„í•œ ë‹¨ì • í‘œí˜„ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "confidenceì— ë§ëŠ” ì™„í™” í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš” (>=0.80: 'ê°œì—°ì„±ì´ ë§¤ìš° ë†’ìŒ', 0.50-0.79: 'ê°œì—°ì„±ì´ ë†’ìŒ', <0.50: 'ê°€ì„¤ ìˆ˜ì¤€')",
      "evidence_refs": ["evidence.sources[].confidence"]
    },
    {
      "rule_id": "REL-400",
      "severity": "error",
      "when": "Relation íƒì§€ ê²°ê³¼ì™€ ëª¨ìˆœë˜ëŠ” ë‹¨ì • (ì˜ˆ: ì¶© ê´€ê³„ ì—†ëŠ”ë° 'ì¶©ì´ ìˆë‹¤' ì£¼ì¥)",
      "check_type": "match",
      "action": "revise",
      "reason_code": "REL-MISMATCH",
      "message_ko": "ê´€ê³„ ë¶„ì„ ê²°ê³¼ì™€ ëª¨ìˆœë˜ëŠ” ì£¼ì¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "evidence.derived.relations ê²°ê³¼ì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •í•˜ì„¸ìš”",
      "evidence_refs": ["evidence.derived.relations"]
    },
    {
      "rule_id": "SIG-500",
      "severity": "error",
      "when": "ì…ë ¥ signatures.policy_refs[] ì„œëª… ê²€ì¦ ì‹¤íŒ¨",
      "check_type": "calc",
      "action": "deny",
      "reason_code": "POLICY-SIG-MISMATCH",
      "message_ko": "ì •ì±… ì„œëª… ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "ì •ì±… íŒŒì¼ì˜ ë¬´ê²°ì„±ì„ í™•ì¸í•˜ê³  ì¬ìš”ì²­í•˜ì„¸ìš”",
      "evidence_refs": ["evidence.signatures.policy_refs"]
    },
    {
      "rule_id": "PII-600",
      "severity": "warn",
      "when": "PII (ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ìƒì„¸ì£¼ì†Œ, ì£¼ë¯¼ë²ˆí˜¸ ìœ ì‚¬) í¬í•¨",
      "check_type": "match",
      "action": "revise",
      "reason_code": "PII-DETECTED",
      "message_ko": "ê°œì¸ ì‹ë³„ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "PIIëŠ” ë§ˆìŠ¤í‚¹ ë˜ëŠ” ì œê±° í›„ ì‘ë‹µí•˜ì„¸ìš” (redactions[] í™œìš©)",
      "evidence_refs": []
    },
    {
      "rule_id": "KO-700",
      "severity": "warn",
      "when": "KO-first ë¼ë²¨ ë¯¸ì‚¬ìš© ë˜ëŠ” ì˜ë¬¸ ì „ìš© ì‘ë‹µ",
      "check_type": "match",
      "action": "revise",
      "reason_code": "LABEL-NONCOMPLIANT",
      "message_ko": "í•œêµ­ì–´ ìš°ì„ (KO-first) ë¼ë²¨ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤",
      "remediation_hint_ko": "ëª¨ë“  enum/code í•„ë“œì— *_ko ë³‘í–‰ ë¼ë²¨ì„ ì œê³µí•˜ì„¸ìš”",
      "evidence_refs": []
    },
    {
      "rule_id": "AMBIG-800",
      "severity": "warn",
      "when": "ê³ ì „/ì •ì±… ì¶œì²˜ ë¼ë²¨ ë˜ëŠ” ê·¼ê±° ëˆ„ë½",
      "check_type": "match",
      "action": "revise",
      "reason_code": "AMBIG-SOURCE",
      "message_ko": "ê³ ì „ ë˜ëŠ” ì •ì±… ì¶œì²˜ ê·¼ê±°ê°€ ëª¨í˜¸í•©ë‹ˆë‹¤",
      "remediation_hint_ko": "ê³ ì „ ì¸ìš© ì‹œ ì¶œì „(ì˜ˆ: 'ìí‰ì§„ì „'), ì •ì±… ì¸ìš© ì‹œ ì •ì±…ëª…(ì˜ˆ: 'strength_policy_v2')ì„ ëª…ì‹œí•˜ì„¸ìš”",
      "evidence_refs": []
    }
  ],
  "modality_mapping": [
    {
      "confidence_min": 0.8,
      "confidence_max": 1.0,
      "label_ko": "ê°œì—°ì„±ì´ ë§¤ìš° ë†’ìŒ",
      "allowed_expressions": [
        "~ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤",
        "~ë¡œ í•´ì„ë©ë‹ˆë‹¤"
      ]
    },
    {
      "confidence_min": 0.5,
      "confidence_max": 0.79,
      "label_ko": "ê°œì—°ì„±ì´ ë†’ìŒ",
      "allowed_expressions": [
        "~ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤",
        "~ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
      ]
    },
    {
      "confidence_min": 0.0,
      "confidence_max": 0.49,
      "label_ko": "ê°€ì„¤ ìˆ˜ì¤€",
      "allowed_expressions": [
        "~ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤",
        "~ë¡œ ì¶”ì •ë©ë‹ˆë‹¤ (í™•ì • ì•„ë‹˜)"
      ]
    }
  ],
  "pii_patterns": [
    {
      "type": "phone_kr",
      "pattern": "01[0-9]-?[0-9]{3,4}-?[0-9]{4}"
    },
    {
      "type": "email",
      "pattern": "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}"
    },
    {
      "type": "address_detailed",
      "pattern": "(ì‹œ|êµ¬|ë™|ë¡œ|ê¸¸)\\s*[0-9-]+.*í˜¸"
    },
    {
      "type": "ssn_like",
      "pattern": "[0-9]{6}-?[1-4][0-9]{6}"
    }
  ],
  "decision_algorithm": "ìˆœì°¨ í‰ê°€: evaluation_order ìˆœì„œëŒ€ë¡œ ê° ê·œì¹™ í‰ê°€ â†’ ì²« ìœ„ë°˜(fail) ì‹œ í•´ë‹¹ action ì¦‰ì‹œ ë°˜í™˜ â†’ logs.trace[]ì— ëª¨ë“  ê·œì¹™ ê²°ê³¼ ê¸°ë¡ â†’ decision/reasons/remediations ìƒì„± â†’ risk_score ê³„ì‚°",
  "risk_score_heuristics": "ìœ„ë°˜ ê°¯ìˆ˜ Ã— 10 + (severity='error' ì‹œ +20, 'warn' ì‹œ +5) ê°€ì¤‘ í•©ì‚°, ìµœëŒ€ 100ìœ¼ë¡œ í´ë¨í”„"
}
```

---

## Schemas

### Input Schema: `schema/llm_guard_input_schema_v1.json`

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "/schemas/llm_guard_input_v1.schema.json",
  "title": "llm_guard_input",
  "type": "object",
  "required": ["evidence", "candidate_answer"],
  "properties": {
    "evidence": {
      "type": "object",
      "required": ["case_id", "pillars", "derived", "sources", "signatures"],
      "properties": {
        "case_id": {"type": "string", "minLength": 1},
        "pillars": {
          "type": "object",
          "required": ["year", "month", "day", "hour"],
          "properties": {
            "year": {"type": "string", "pattern": "^[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]$"},
            "month": {"type": "string", "pattern": "^[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]$"},
            "day": {"type": "string", "pattern": "^[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]$"},
            "hour": {"type": ["string", "null"], "pattern": "^[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]$"}
          }
        },
        "derived": {
          "type": "object",
          "properties": {
            "strength": {"type": "object", "properties": {"level": {"type": "string"}, "score": {"type": "number"}}},
            "shensha": {"type": "array", "items": {"type": "object"}},
            "relations": {"type": "object", "properties": {"he6": {"type": "array"}, "sanhe": {"type": "array"}, "chong": {"type": "array"}, "xing": {"type": "array"}}},
            "wuxing_adjust": {"type": "object"},
            "void": {"type": "object", "properties": {"kong": {"type": "array", "items": {"type": "string"}}}}
          }
        },
        "sources": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["evidence_id", "type", "value", "confidence"],
            "properties": {
              "evidence_id": {"type": "string", "minLength": 1},
              "type": {"type": "string", "enum": ["engine_output", "policy_rule", "classic_text", "calculation"]},
              "value": {"type": "object"},
              "confidence": {"type": "number", "minimum": 0, "maximum": 1},
              "trace": {"type": "array", "items": {"type": "string"}}
            }
          }
        },
        "signatures": {
          "type": "object",
          "required": ["canonical_sha256", "policy_refs"],
          "properties": {
            "canonical_sha256": {"type": "string", "pattern": "^[0-9a-f]{64}$"},
            "policy_refs": {"type": "array", "items": {"type": "string", "pattern": "^[0-9a-f]{64}$"}}
          }
        }
      }
    },
    "candidate_answer": {"type": ["string", "object"]},
    "requested_capabilities": {"type": "array", "items": {"type": "string"}},
    "policy_context": {
      "type": "object",
      "properties": {
        "allowed_claim_types": {"type": "array"},
        "forbidden_patterns": {"type": "array"},
        "locale": {"type": "string", "pattern": "^ko-KR$"},
        "ui_mode": {"type": "string", "enum": ["explainable", "compact"]}
      }
    },
    "runtime_info": {
      "type": "object",
      "properties": {
        "model_name": {"type": "string"},
        "prompt_id": {"type": "string"},
        "timestamp": {"type": "string", "pattern": "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$"}
      }
    }
  }
}
```

### Output Schema: `schema/llm_guard_output_schema_v1.json`

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "/schemas/llm_guard_output_v1.schema.json",
  "title": "llm_guard_output",
  "type": "object",
  "required": ["decision", "reasons", "risk_score", "policy_snapshot_sha256", "logs"],
  "properties": {
    "decision": {"type": "string", "enum": ["allow", "revise", "deny"]},
    "reasons": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["code", "message_ko"],
        "properties": {
          "code": {"type": "string", "enum": ["INPUT-INVALID", "LLM-CLAIM-NOEVID", "OUT-OF-SCOPE", "MODALITY-OVERCLAIM", "REL-MISMATCH", "POLICY-SIG-MISMATCH", "PII-DETECTED", "LABEL-NONCOMPLIANT", "AMBIG-SOURCE"]},
          "message_ko": {"type": "string", "minLength": 1}
        }
      }
    },
    "remediations": {"type": "array", "items": {"type": "string", "minLength": 1}},
    "citations": {"type": "array", "items": {"type": "string", "minLength": 1}},
    "redactions": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["type", "value", "rule_id"],
        "properties": {
          "type": {"type": "string"},
          "value": {"type": "string"},
          "rule_id": {"type": "string"}
        }
      }
    },
    "risk_score": {"type": "number", "minimum": 0, "maximum": 100},
    "policy_snapshot_sha256": {"type": "string", "pattern": "^[0-9a-f]{64}$"},
    "logs": {
      "type": "object",
      "required": ["trace"],
      "properties": {
        "trace": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["rule_id", "result"],
            "properties": {
              "rule_id": {"type": "string"},
              "result": {"type": "string", "enum": ["pass", "fail"]},
              "evidence_refs": {"type": "array", "items": {"type": "string"}},
              "note_ko": {"type": "string"}
            }
          }
        }
      }
    }
  }
}
```

---

## Test Cases

### File: `tests/llm_guard_cases_v1.jsonl`

**18 test cases** (JSONL format, one per line):

**Allow (6 cases):**
1. Normal with evidence citation + proper modality
2. Proper modality expression (confidence 0.72 â†’ "ê°œì—°ì„±ì´ ë†’ìŒ")
3. Proper Korean labels (bucket_ko present)
4. Relation match (chong detected â†’ claim "ìì˜¤ì¶© ìˆìŒ")
5. Low confidence hypothesis tone (confidence 0.45 â†’ "ê°€ì„¤ ìˆ˜ì¤€")
6. Proper citation list

**Revise (6 cases):**
7. No evidence â†’ `LLM-CLAIM-NOEVID`
8. Modality overclaim â†’ `MODALITY-OVERCLAIM` (confidence 0.48 but uses "í™•ì‹¤íˆ")
9. Relation mismatch â†’ `REL-MISMATCH` (evidence: no chong, claim: has chong)
10. PII phone â†’ `PII-DETECTED`
11. No Korean labels â†’ `LABEL-NONCOMPLIANT`
12. Ambiguous source â†’ `AMBIG-SOURCE`

**Deny (6 cases):**
13. Out-of-scope medical â†’ `OUT-OF-SCOPE`
14. Out-of-scope birth time â†’ `OUT-OF-SCOPE`
15. Out-of-scope death prediction â†’ `OUT-OF-SCOPE`
16. Invalid input â†’ `INPUT-INVALID`
17. Policy signature fail â†’ `POLICY-SIG-MISMATCH`
18. Severe PII (SSN) â†’ `PII-DETECTED`

**Format**: Each line is a JSON object with `name`, `input`, `expected` fields.

---

## Examples

### Example 1: Allow - Evidence Citation + Proper Modality

**Input:**
```json
{
  "evidence": {
    "case_id": "good-001",
    "pillars": {"year": "åºšè¾°", "month": "ä¹™é…‰", "day": "ä¹™äº¥", "hour": "è¾›å·³"},
    "derived": {"strength": {"level": "ì‹ ì•½", "score": 35}},
    "sources": [
      {
        "evidence_id": "STR-001",
        "type": "engine_output",
        "value": {"bucket": "ì‹ ì•½", "score": 35},
        "confidence": 0.85,
        "trace": ["strength_policy_v2"]
      }
    ],
    "signatures": {
      "canonical_sha256": "abc123...",
      "policy_refs": ["def456..."]
    }
  },
  "candidate_answer": "ì¼ê°„ì´ ì•½í•˜ë¯€ë¡œ(STR-001) ê°œì—°ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤",
  "policy_context": {"locale": "ko-KR", "ui_mode": "explainable"}
}
```

**Output:**
```json
{
  "decision": "allow",
  "reasons": [],
  "citations": ["STR-001"],
  "risk_score": 0,
  "policy_snapshot_sha256": "a4dec835...",
  "logs": {
    "trace": [
      {"rule_id": "STRUCT-000", "result": "pass"},
      {"rule_id": "EVID-BIND-100", "result": "pass", "evidence_refs": ["STR-001"]},
      {"rule_id": "MODAL-300", "result": "pass", "note_ko": "confidence 0.85 â†’ 'ë§¤ìš° ë†’ìŒ' ì í•©"}
    ]
  }
}
```

---

### Example 2: Revise - No Evidence

**Input:**
```json
{
  "evidence": {
    "case_id": "bad-001",
    "pillars": {"year": "åºšè¾°", "month": "ä¹™é…‰", "day": "ä¹™äº¥", "hour": "è¾›å·³"},
    "derived": {"strength": {"level": "ì‹ ì•½", "score": 35}},
    "sources": [
      {
        "evidence_id": "STR-007",
        "type": "engine_output",
        "value": {"bucket": "ì‹ ì•½", "score": 35},
        "confidence": 0.85,
        "trace": ["strength_policy_v2"]
      }
    ],
    "signatures": {
      "canonical_sha256": "abc129...",
      "policy_refs": ["def462..."]
    }
  },
  "candidate_answer": "ì¼ê°„ì´ ì•½í•˜ê³  ìš©ì‹ ì€ ê¸ˆì…ë‹ˆë‹¤",
  "policy_context": {"locale": "ko-KR"}
}
```

**Output:**
```json
{
  "decision": "revise",
  "reasons": [
    {
      "code": "LLM-CLAIM-NOEVID",
      "message_ko": "ê·¼ê±° ì—†ëŠ” ì‚¬ì‹¤ ì£¼ì¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
    }
  ],
  "remediations": [
    "'ìš©ì‹ ì€ ê¸ˆì…ë‹ˆë‹¤' ì£¼ì¥ì— ëŒ€í•œ evidence_idë¥¼ ì¸ìš©í•˜ì„¸ìš”"
  ],
  "citations": ["STR-007"],
  "risk_score": 30,
  "policy_snapshot_sha256": "a4dec835...",
  "logs": {
    "trace": [
      {"rule_id": "EVID-BIND-100", "result": "fail", "note_ko": "'ìš©ì‹ ì€ ê¸ˆ' ì£¼ì¥ì— ê·¼ê±° ì—†ìŒ"}
    ]
  }
}
```

---

### Example 3: Deny - Out of Scope (Medical)

**Input:**
```json
{
  "evidence": {
    "case_id": "deny-001",
    "pillars": {"year": "åºšè¾°", "month": "ä¹™é…‰", "day": "ä¹™äº¥", "hour": "è¾›å·³"},
    "derived": {"strength": {"level": "ì‹ ì•½", "score": 35}},
    "sources": [
      {
        "evidence_id": "STR-013",
        "type": "engine_output",
        "value": {"bucket": "ì‹ ì•½", "score": 35},
        "confidence": 0.85,
        "trace": ["strength_policy_v2"]
      }
    ],
    "signatures": {
      "canonical_sha256": "abc135...",
      "policy_refs": ["def468..."]
    }
  },
  "candidate_answer": "ì¼ê°„ì´ ì•½í•˜ë¯€ë¡œ ê°„ ì§ˆí™˜ì´ ìˆìŠµë‹ˆë‹¤",
  "policy_context": {"locale": "ko-KR"},
  "requested_capabilities": ["ì˜ë£Œ ì§„ë‹¨"]
}
```

**Output:**
```json
{
  "decision": "deny",
  "reasons": [
    {
      "code": "OUT-OF-SCOPE",
      "message_ko": "í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë²”ìœ„ì˜ ìš”ì²­ì…ë‹ˆë‹¤"
    }
  ],
  "remediations": [
    "ì˜ë£Œ ì§„ë‹¨ì€ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
  ],
  "risk_score": 50,
  "policy_snapshot_sha256": "a4dec835...",
  "logs": {
    "trace": [
      {"rule_id": "SCOPE-200", "result": "fail", "note_ko": "ì˜ë£Œ ì§„ë‹¨ ìš”ì²­ ê°ì§€"}
    ]
  }
}
```

---

## Implementation Roadmap

### Phase 1: Rule Evaluators (Estimated: 2-3 days)

Create individual evaluator classes for each rule:

```python
class StructValidator:
    """STRUCT-000: Schema validation"""
    def evaluate(input_data: dict) -> RuleResult:
        # Validate against llm_guard_input_schema_v1.json
        # Return pass/fail

class EvidenceBindingChecker:
    """EVID-BIND-100: Evidence binding check"""
    def evaluate(candidate_answer: str, sources: list) -> RuleResult:
        # Extract claims from candidate_answer
        # Check each claim has corresponding evidence_id
        # Return pass/fail with missing evidence_ids

class ScopeGuard:
    """SCOPE-200: Out-of-scope request detector"""
    def evaluate(candidate_answer: str, capabilities: list) -> RuleResult:
        # Check for forbidden keywords: ì˜ë£Œ/ì§„ë‹¨/ì§ˆí™˜/ë³‘ì›/ì¹˜ë£Œ
        # Check for forbidden keywords: ë²•ë¥ /ì†Œì†¡/ê³„ì•½/íˆ¬ì/ì£¼ì‹
        # Check for forbidden keywords: ì¶œìƒì‹œê°/ì¶”ì •/ì‚¬ë§ì¼/ì˜ˆì¸¡
        # Return pass/fail

class ModalityMatcher:
    """MODAL-300: Confidence-to-expression matcher"""
    def evaluate(candidate_answer: str, sources: list) -> RuleResult:
        # Extract expressions from candidate_answer
        # Check confidence ranges (0.8-1.0, 0.5-0.79, 0.0-0.49)
        # Match expression tone to confidence
        # Return pass/fail with overconfident expressions

class RelationValidator:
    """REL-400: Relation consistency checker"""
    def evaluate(candidate_answer: str, relations: dict) -> RuleResult:
        # Extract relation claims from candidate_answer
        # Compare with evidence.derived.relations
        # Return pass/fail with mismatches

class SignatureVerifier:
    """SIG-500: Policy signature verification"""
    def evaluate(policy_refs: list) -> RuleResult:
        # Use Policy Signature Auditor to verify each hash
        # Return pass/fail with invalid hashes

class PIIDetector:
    """PII-600: PII pattern detection"""
    def evaluate(candidate_answer: str) -> RuleResult:
        # Apply 4 regex patterns (phone_kr, email, address, ssn_like)
        # Return pass/fail with redactions[]

class KoLabelChecker:
    """KO-700: Korean label compliance"""
    def evaluate(candidate_answer: str, sources: list) -> RuleResult:
        # Check for *_ko fields in sources
        # Check for Korean vs English usage
        # Return pass/fail

class SourceCitationChecker:
    """AMBIG-800: Source citation clarity"""
    def evaluate(candidate_answer: str) -> RuleResult:
        # Check for vague phrases: "ê³ ì „ì—ì„œ", "ì •ì±…ì— ë”°ë¥´ë©´"
        # Require specific citations: "ìí‰ì§„ì „", "strength_policy_v2"
        # Return pass/fail
```

### Phase 2: Decision Engine (Estimated: 1 day)

```python
class LLMGuardV1:
    """Full LLM Guard v1.0 implementation"""

    def __init__(self, policy_path: str):
        self.policy = self._load_policy(policy_path)
        self.evaluators = {
            "STRUCT-000": StructValidator(),
            "EVID-BIND-100": EvidenceBindingChecker(),
            "SCOPE-200": ScopeGuard(),
            "MODAL-300": ModalityMatcher(),
            "REL-400": RelationValidator(),
            "SIG-500": SignatureVerifier(),
            "PII-600": PIIDetector(),
            "KO-700": KoLabelChecker(),
            "AMBIG-800": SourceCitationChecker()
        }

    def evaluate(self, input_data: dict) -> dict:
        """Main evaluation entry point"""
        logs_trace = []
        risk_score = 0

        # Sequential evaluation in policy order
        for rule_id in self.policy["evaluation_order"]:
            rule = self._get_rule(rule_id)
            evaluator = self.evaluators[rule_id]

            # Evaluate rule
            result = evaluator.evaluate(input_data)

            # Log result
            logs_trace.append({
                "rule_id": rule_id,
                "result": "pass" if result.passed else "fail",
                "evidence_refs": result.evidence_refs,
                "note_ko": result.note
            })

            # First failure â†’ immediate action
            if not result.passed:
                risk_score = self._calculate_risk(logs_trace, rule)
                return {
                    "decision": rule["action"],
                    "reasons": [{"code": rule["reason_code"], "message_ko": rule["message_ko"]}],
                    "remediations": [rule["remediation_hint_ko"]],
                    "citations": self._extract_citations(input_data),
                    "redactions": result.redactions,
                    "risk_score": risk_score,
                    "policy_snapshot_sha256": self.policy["policy_signature"],
                    "logs": {"trace": logs_trace}
                }

        # All passed
        return {
            "decision": "allow",
            "reasons": [],
            "citations": self._extract_citations(input_data),
            "risk_score": 0,
            "policy_snapshot_sha256": self.policy["policy_signature"],
            "logs": {"trace": logs_trace}
        }

    def _calculate_risk(self, logs_trace: list, failed_rule: dict) -> int:
        """Calculate risk score: violations Ã— 10 + severity weights"""
        violations = sum(1 for log in logs_trace if log["result"] == "fail")
        severity_weight = 20 if failed_rule["severity"] == "error" else 5
        return min(violations * 10 + severity_weight, 100)
```

### Phase 3: Test Runner (Estimated: 0.5 day)

```python
def run_llm_guard_tests(jsonl_path: str, guard: LLMGuardV1):
    """Run all 18 test cases from JSONL file"""
    cases = load_jsonl(jsonl_path)
    passed = 0
    failed = 0

    for case in cases:
        result = guard.evaluate(case["input"])
        expected = case["expected"]

        # Validate decision
        if result["decision"] == expected["decision"]:
            # Validate reasons codes match
            if all(r["code"] in [e["code"] for e in expected.get("reasons", [])] for r in result["reasons"]):
                passed += 1
                print(f"âœ… {case['name']}")
            else:
                failed += 1
                print(f"âŒ {case['name']} - reasons mismatch")
        else:
            failed += 1
            print(f"âŒ {case['name']} - decision mismatch")

    print(f"\n{passed}/{len(cases)} tests passed")
```

### Phase 4: Integration (Estimated: 1 day)

- Replace minimal `llm_guard.py` with `llm_guard_v1.py`
- Update `AnalysisEngine` to use full guard
- Add guard to LLM polish pipeline
- Update CI to run 18 test cases

---

## File Locations

```
/Users/yujumyeong/coding/ projects/ì‚¬ì£¼/

Policy & Schemas:
â”œâ”€â”€ policy/llm_guard_policy_v1.json                    (181 lines, signed)
â”œâ”€â”€ schema/llm_guard_input_schema_v1.json              (264 lines)
â”œâ”€â”€ schema/llm_guard_output_schema_v1.json             (138 lines)

Current Implementation:
â”œâ”€â”€ services/analysis-service/app/core/llm_guard.py    (63 lines, minimal)
â”œâ”€â”€ services/analysis-service/tests/test_llm_guard.py  (35 lines, 2/2 passing)

Test Cases & Examples:
â”œâ”€â”€ tests/llm_guard_cases_v1.jsonl                     (18 cases)
â”œâ”€â”€ samples/llm_guard_io_examples_v1.md                (516 lines, detailed examples)

Documentation:
â”œâ”€â”€ CHANGELOG_llm_guard_v1.md                          (version history)
â”œâ”€â”€ docs/policy-prompts/30_llm/llm_guard_v1_prompt.md  (implementation guide)
â””â”€â”€ LLM_GUARD_COMPLETE_HANDOVER.md                     (this document)
```

---

## Quick Start Guide

### For AI Agent Taking Over:

1. **Read this document** - Everything you need is here
2. **Load policy** - `policy/llm_guard_policy_v1.json`
3. **Review schemas** - Input/output structure
4. **Study test cases** - `tests/llm_guard_cases_v1.jsonl` (18 scenarios)
5. **Implement evaluators** - Follow Phase 1 roadmap (9 rule classes)
6. **Build decision engine** - Follow Phase 2 roadmap (sequential evaluation)
7. **Run tests** - Validate against 18 JSONL cases
8. **Integrate** - Replace minimal guard with full v1.0

### Key Principles:

- **Policy is law**: Don't deviate from policy specifications
- **Sequential evaluation**: Stop at first failure
- **Log everything**: `logs.trace[]` records all rule results
- **Risk scoring**: `violations Ã— 10 + severity_weight`
- **UI modes**: `explainable` (full) vs `compact` (first only)

---

## Contact & Support

**Policy Signature**: `a4dec83545592db3f3d7f3bdfaaf556a325e2c78f5ce7a39813ec6a077960ad2`
**Verification**: Use Policy Signature Auditor v1.0 to verify integrity

```bash
python policy_signature_auditor/psa_cli.py verify policy/llm_guard_policy_v1.json --strict
```

---

**End of Handover Document**

Generated: 2025-10-09 KST
Version: 1.0.0
Total Lines: ~3,500 (policy + schemas + code + tests + docs)
