#!/usr/bin/env python3
"""
Stage-2 Audit Runner (offline)
Comprehensive pre-Stage-3 integrity check for Saju project
"""
import glob
import hashlib
import json
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

ROOT = Path.cwd()
REPORTS = ROOT / "reports"
REPORTS.mkdir(exist_ok=True, parents=True)


def find_files(patterns: List[str]) -> List[Path]:
    """Find files matching glob patterns"""
    out = []
    for pat in patterns:
        out.extend([Path(p) for p in glob.glob(pat, recursive=True)])
    # unique & existing
    return sorted(set([p for p in out if p.exists()]))


def run(cmd: List[str], check: bool = False) -> Tuple[int, str, str]:
    """Run shell command and capture output"""
    try:
        proc = subprocess.run(cmd, capture_output=True, text=True, cwd=ROOT)
        return proc.returncode, proc.stdout.strip(), proc.stderr.strip()
    except Exception as e:
        return 127, "", str(e)


def has_psa() -> bool:
    """Check if Policy Signature Auditor exists"""
    return (ROOT / "policy_signature_auditor" / "psa_cli.py").exists()


def verify_signatures() -> Tuple[str, int, int, int]:
    """Verify policy signatures and generate schema sidecar hashes"""
    report = []

    # Separate policy files (require signature) from schema files (sidecar hash only)
    policy_targets = find_files(["policy/**/*.json", "core/policy_engines/**/policy/*.json"])
    schema_targets = find_files(["schema/**/*.json"])

    if not policy_targets and not schema_targets:
        report.append("‚ö†Ô∏è  No JSON policy/schema files found.")
        return "\n".join(report), 0, 0, 0

    ok = mis = err = 0

    if has_psa():
        # 1) Policy files: signature verification
        report.append("## Policy File Signatures")
        for p in policy_targets:
            rc, out, stderr = run(
                [
                    sys.executable,
                    str(ROOT / "policy_signature_auditor" / "psa_cli.py"),
                    "verify",
                    str(p),
                ]
            )

            try:
                rel_path = p.relative_to(ROOT)
            except ValueError:
                rel_path = p

            if rc == 0 and "Valid" in out:
                ok += 1
                report.append(f"‚úÖ OK   : {rel_path}")
            elif "UNSIGNED" in out or "Expected: UNSIGNED" in stderr:
                mis += 1
                report.append(f"‚ö†Ô∏è  UNSIGNED: {rel_path}")
            else:
                err += 1
                report.append(f"‚ùå ERROR : {rel_path}")
                if out:
                    report.append(f"         {out}")

        # 2) Schema files: sidecar hash generation (no signature enforcement)
        report.append("\n## Schema File Sidecar Hashes")
        for s in schema_targets:
            try:
                rel_path = s.relative_to(ROOT)
            except ValueError:
                rel_path = s

            # Generate SHA-256 sidecar hash
            try:
                content = s.read_bytes()
                hash_val = hashlib.sha256(content).hexdigest()
                sidecar_path = Path(str(s) + ".sha256")
                sidecar_path.write_text(hash_val, encoding="utf-8")
                report.append(f"üìÑ HASH  : {rel_path} ‚Üí {hash_val[:16]}...")
            except Exception as e:
                report.append(f"‚ùå HASH ERROR: {rel_path} :: {e}")
    else:
        report.append("‚ö†Ô∏è  PSA not found. Skipping verify.")
        report.append(
            "   Add policy_signature_auditor/psa_cli.py to enable signature verification."
        )

    return "\n".join(report), ok, mis, err


def load_json(p: Path) -> Tuple[Optional[Dict], Optional[str]]:
    """Load JSON file safely"""
    try:
        return json.loads(p.read_text(encoding="utf-8")), None
    except Exception as e:
        return None, str(e)


def jsonl_lines(p: Path) -> List[Dict]:
    """Parse JSONL file"""
    lines = []
    for line in p.read_text(encoding="utf-8").splitlines():
        if line.strip():
            try:
                lines.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return lines


def write_report(name: str, content: str):
    """Write report to file"""
    (REPORTS / name).write_text(content, encoding="utf-8")
    print(f"‚úÖ Generated: reports/{name}")


def inventory() -> str:
    """Generate repository inventory"""
    top = []
    dirs = [
        "policy",
        "schema",
        "core",
        "services",
        "docs",
        "tests",
        "report",
        "core/policy_engines",
        "policy_signature_auditor",
    ]

    for d in dirs:
        p = ROOT / d
        if p.exists():
            files = list(p.rglob("*"))[:50]  # Limit to 50 files per dir
            file_list = "\n".join([f"  - {f.name}" for f in files[:20] if f.is_file()])
            count = len([f for f in files if f.is_file()])
            top.append(f"## {d}\n\n**Files found:** {count}\n\n{file_list}\n")

    return "\n\n".join(top) if top else "‚ö†Ô∏è  No standard directories found."


def rule_matrix() -> str:
    """Generate LLM Guard rule coverage matrix"""
    cases_v10 = find_files(["tests/llm_guard_cases_v1.jsonl"])
    cases_v11 = find_files(["tests/llm_guard_v1.1_cases.jsonl", "tests/llm_guard_v1_1_cases.jsonl"])

    lines = [
        "# LLM Guard Rule ‚Üî Test Coverage Matrix\n",
        "**Primary Test Suite:** v1.1 (single source of truth)",
        "**v1.0 Status:** Legacy/regression only (heuristic coverage detection)\n",
    ]

    rules_v10 = [
        "STRUCT-000",
        "EVID-BIND-100",
        "SCOPE-200",
        "MODAL-300",
        "REL-400",
        "SIG-500",
        "PII-600",
        "KO-700",
        "AMBIG-800",
    ]

    rules_v11 = rules_v10 + [
        "CONF-LOW-310",
        "REL-OVERWEIGHT-410",
        "CONSIST-450",
        "YONGSHIN-UNSUPPORTED-460",
    ]

    for label, cases, rules in [("v1.0", cases_v10, rules_v10), ("v1.1", cases_v11, rules_v11)]:
        covered = {r: [] for r in rules}

        if cases:
            case_file = cases[0]
            try:
                rel_path = case_file.relative_to(ROOT)
            except ValueError:
                rel_path = case_file.name
            lines.append(f"\n## {label} ({rel_path})\n")

            L = jsonl_lines(case_file)
            for i, c in enumerate(L):
                test_id = c.get("test_id", c.get("name", f"case-{i}"))

                # Check both "expected" and "expected_violations" formats
                expected = c.get("expected", {})
                violations = expected.get("violations", c.get("expected_violations", []))

                # v1.0 compatibility: if no violations, try heuristic matching from name
                if not violations and label == "v1.0":
                    name_upper = test_id.upper()
                    # Heuristic mapping based on test case naming patterns
                    heuristic_map = {
                        "STRUCT": "STRUCT-000",
                        "EVIDENCE": "EVID-BIND-100",
                        "SCOPE": "SCOPE-200",
                        "MODAL": "MODAL-300",
                        "REL": "REL-400",
                        "SIGNATURE": "SIG-500",
                        "PII": "PII-600",
                        "KO": "KO-700",
                        "KOREAN": "KO-700",
                        "AMBIG": "AMBIG-800",
                    }
                    for keyword, rule_id in heuristic_map.items():
                        if keyword in name_upper and rule_id in covered:
                            covered[rule_id].append(f"{test_id} (heuristic)")
                            break
                else:
                    # Normal path: extract rule_id from violations
                    for v in violations:
                        rule_id = v.get("rule_id", "")
                        if rule_id in covered:
                            covered[rule_id].append(test_id)

            # Report coverage
            lines.append("| Rule ID | Coverage | Test Cases |")
            lines.append("|---------|----------|------------|")

            for r in rules:
                if covered[r]:
                    mark = "‚úÖ"
                    cases_str = ", ".join(covered[r][:3])
                    if len(covered[r]) > 3:
                        cases_str += f" (+{len(covered[r])-3} more)"
                else:
                    mark = "‚ö†Ô∏è"
                    cases_str = "**NO COVERAGE**"

                lines.append(f"| {r} | {mark} | {cases_str} |")
        else:
            lines.append(f"\n## {label}\n")
            lines.append("‚ö†Ô∏è  **No test file found.**\n")

    return "\n".join(lines)


def schema_conformance() -> str:
    """Check schema file presence and basic structure"""
    out = ["# Schema Conformance Report\n"]

    schemas = {
        "v1.0 Input": "schema/llm_guard_input_schema_v1.json",
        "v1.0 Output": "schema/llm_guard_output_schema_v1.json",
        "v1.1 Input": "schema/llm_guard_input_v1.1.json",
        "v1.1 Output": "schema/llm_guard_output_v1.1.json",
        "Gyeokguk Input": "schema/gyeokguk_input_schema_v1.json",
        "Gyeokguk Output": "schema/gyeokguk_output_schema_v1.json",
    }

    out.append("## Schema Files\n")
    out.append("| Schema | Status | Path |")
    out.append("|--------|--------|------|")

    for name, path in schemas.items():
        p = ROOT / path
        status = "‚úÖ FOUND" if p.exists() else "‚ö†Ô∏è  MISSING"
        out.append(f"| {name} | {status} | `{path}` |")

    # Check sample evidence
    out.append("\n## Sample Evidence\n")
    sample_paths = [
        "report/saju/sample_evidence.json",
        "samples/llm_guard_v1.1_io_examples.md",
        "samples/gyeokguk_io_examples_v1.md",
    ]

    out.append("| Sample | Status | Path |")
    out.append("|--------|--------|------|")

    for path in sample_paths:
        p = ROOT / path
        status = "‚úÖ FOUND" if p.exists() else "‚ö†Ô∏è  MISSING"
        out.append(f"| {path.split('/')[-1]} | {status} | `{path}` |")

    return "\n".join(out)


def cross_engine_consistency_probe() -> str:
    """Static probe for cross-engine consistency (no runtime)"""
    lines = [
        "# Cross-Engine Consistency Probe (Static Analysis)\n",
        "This probe checks field presence and plausible value ranges without runtime execution.\n",
    ]

    # Check if v1.1 input schema has engine_summaries
    v11_input = ROOT / "schema/llm_guard_input_v1.1.json"

    if v11_input.exists():
        data, err = load_json(v11_input)
        if data:
            props = data.get("properties", {})
            engine_sum = props.get("engine_summaries", {})

            lines.append("## v1.1 Input Schema Analysis\n")
            lines.append("‚úÖ Schema file found\n")

            if engine_sum:
                lines.append("‚úÖ `engine_summaries` field present\n")

                req_props = engine_sum.get("properties", {})
                checks = {
                    "strength": "Strength evaluation",
                    "relation_summary": "Relation analysis",
                    "yongshin_result": "Yongshin selection",
                    "climate": "Climate evaluation",
                }

                lines.append("\n### Required Fields:\n")
                for field, desc in checks.items():
                    status = "‚úÖ" if field in req_props else "‚ùå"
                    lines.append(f"- {status} `{field}`: {desc}")
            else:
                lines.append("‚ùå `engine_summaries` field NOT FOUND in schema\n")
        else:
            lines.append(f"‚ùå Error loading schema: {err}\n")
    else:
        lines.append("‚ö†Ô∏è  v1.1 input schema not found\n")

    # Check test cases for cross-engine fields
    lines.append("\n## Test Case Analysis\n")
    test_file = ROOT / "tests/llm_guard_v1.1_cases.jsonl"

    if test_file.exists():
        cases = jsonl_lines(test_file)
        lines.append(f"‚úÖ Found {len(cases)} test cases\n")

        # Check for engine_summaries in test inputs
        has_engine_sum = 0
        for c in cases:
            inp = c.get("input", {})
            if "engine_summaries" in inp:
                has_engine_sum += 1

        lines.append(f"- Test cases with `engine_summaries`: **{has_engine_sum}/{len(cases)}**\n")

        if has_engine_sum < len(cases):
            lines.append(
                f"‚ö†Ô∏è  **GAP:** {len(cases) - has_engine_sum} test cases missing `engine_summaries`\n"
            )
    else:
        lines.append("‚ö†Ô∏è  v1.1 test file not found\n")

    return "\n".join(lines)


def check_policy_signatures_detailed() -> str:
    """Detailed policy signature verification report"""
    lines = ["# Detailed Policy Signature Verification\n"]

    # Check known policies
    known_policies = {
        "LLM Guard v1.0": {
            "path": "policy/llm_guard_policy_v1.json",
            "expected_hash": "a4dec83545592db3f3d7f3bdfaaf556a325e2c78f5ce7a39813ec6a077960ad2",
        },
        "LLM Guard v1.1": {
            "path": "policy/llm_guard_policy_v1.1.json",
            "expected_hash": "591f3f6270efb0907eadd43ff0ea5eeeb1d88fbab45c654af5f669009dc966f7",
        },
        "Relation Weight v1.0": {
            "path": "policy/relation_weight_policy_v1.0.json",
            "expected_hash": "704cf74d323a034ca8f49ceda2659a91e3ff1aed89ee4845950af6eb39df1b67",
        },
        "Yongshin Selector v1.0": {
            "path": "policy/yongshin_selector_policy_v1.json",
            "expected_hash": "e0c95f3fdb1d382b06cd90eca7256f3121d648693d0986f67a5c5d368339cb8c",
        },
        "Gyeokguk Classifier v1.0": {
            "path": "policy/gyeokguk_policy_v1.json",
            "expected_hash": "05089c0a3f0577c1c56214d11a2511c02413cfa1ef1fc39e174129a0fb894aa6",
        },
    }

    lines.append("## Known Policy Signatures\n")
    lines.append("| Policy | Status | Expected Hash | Actual Hash |")
    lines.append("|--------|--------|---------------|-------------|")

    for name, info in known_policies.items():
        path = ROOT / info["path"]
        expected = info["expected_hash"]

        if path.exists():
            data, err = load_json(path)
            if data:
                actual = data.get("policy_signature", "UNSIGNED")
                if actual == expected:
                    status = "‚úÖ MATCH"
                elif actual == "UNSIGNED":
                    status = "‚ö†Ô∏è  UNSIGNED"
                else:
                    status = "‚ùå MISMATCH"

                lines.append(
                    f"| {name} | {status} | `{expected[:16]}...` | `{actual[:16] if actual != 'UNSIGNED' else 'UNSIGNED'}...` |"
                )
            else:
                lines.append(f"| {name} | ‚ùå ERROR | `{expected[:16]}...` | Load error |")
        else:
            lines.append(f"| {name} | ‚ö†Ô∏è  MISSING | `{expected[:16]}...` | File not found |")

    return "\n".join(lines)


def main():
    """Main audit execution"""
    print("=" * 60)
    print("Stage 2 Audit ‚Äî Comprehensive Pre-Stage-3 Check")
    print("=" * 60)

    # 1. Inventory
    print("\n[1/8] Generating repository inventory...")
    inv = inventory()

    # 2. Signature verification
    print("[2/8] Verifying policy signatures...")
    sig_report, ok, mis, err = verify_signatures()
    sig_detailed = check_policy_signatures_detailed()

    # 3. Schema conformance
    print("[3/8] Checking schema conformance...")
    schema_report = schema_conformance()

    # 4. Rule test matrix
    print("[4/8] Generating rule test matrix...")
    matrix = rule_matrix()

    # 5. Cross-engine consistency
    print("[5/8] Probing cross-engine consistency...")
    cross_check = cross_engine_consistency_probe()

    # 6. Write reports
    print("[6/8] Writing reports...")

    # Summary report
    summary = f"""# Stage-2 Audit ‚Äî Executive Summary

**Date:** {subprocess.run(['date'], capture_output=True, text=True).stdout.strip()}
**Repository:** Saju (ÏÇ¨Ï£º) Four Pillars Analysis System
**Audit Scope:** Pre-Stage-3 integrity and consistency check

---

## Quick Stats

- **Policy Signatures:** ‚úÖ {ok} OK, ‚ö†Ô∏è  {mis} UNSIGNED, ‚ùå {err} ERROR
- **Total Policies Checked:** {ok + mis + err}
- **PSA Available:** {'‚úÖ YES' if has_psa() else '‚ùå NO'}

---

## Repository Inventory (Summary)

{inv}

---

## Key Findings

### 1. Policy Signatures
- {ok} policies verified successfully
- {mis} policies need signing
- {err} policies have signature errors

### 2. Schema Files
- See detailed report: `schema_conformance_report.md`

### 3. Test Coverage
- See detailed matrix: `stage2_rule_test_matrix.md`

### 4. Cross-Engine Consistency
- See detailed probe: `cross_engine_consistency.md`

---

## Next Steps

1. Review gap list: `stage2_gap_list.md`
2. Follow action plan: `stage2_action_plan.md`
3. Address HIGH priority items before Stage 3

---

**Generated by:** Stage 2 Audit Runner v1.0
"""

    write_report("stage2_audit_summary.md", summary)

    # Policy signature report
    policy_report = f"""# Policy Signature Verification Report

**Total Files Checked:** {ok + mis + err}
**Results:** ‚úÖ {ok} OK | ‚ö†Ô∏è  {mis} UNSIGNED | ‚ùå {err} ERROR

---

## Verification Results

{sig_report}

---

{sig_detailed}

---

## Recommendations

1. **UNSIGNED policies:** Run Policy Signature Auditor to sign
2. **MISMATCH policies:** Re-sign with current content
3. **ERROR policies:** Check JSON validity and retry

**Command to sign:**
```bash
PYTHONPATH="." python3 policy_signature_auditor/psa_cli.py sign <policy_file>
```
"""

    write_report("policy_signature_report.md", policy_report)
    write_report("schema_conformance_report.md", schema_report)
    write_report("stage2_rule_test_matrix.md", matrix)
    write_report("cross_engine_consistency.md", cross_check)

    # Gap list
    gap_list = """# Stage 2 ‚Äî Gaps & Risks

## HIGH Priority

- [ ] **LLM Guard v1.1 Integration**
  - Risk: v1.1 policy signed but runtime integration pending
  - Impact: Cannot enforce cross-engine consistency rules
  - Owner: Backend team
  - ETA: T+3d

- [ ] **engine_summaries Pipeline**
  - Risk: engine_summaries injection point not confirmed
  - Impact: CONSIST-450, REL-OVERWEIGHT-410, YONGSHIN-UNSUPPORTED-460 cannot trigger
  - Owner: Analysis service team
  - ETA: T+5d

## MEDIUM Priority

- [ ] **Relation Weight conditions_met Propagation**
  - Risk: conditions_met[] may not flow from Relation Weight to Guard input
  - Impact: REL-OVERWEIGHT-410 may have false negatives
  - Owner: Relation engine team
  - ETA: T+7d

- [ ] **Test Coverage Gaps**
  - Risk: Some v1.1 rules lack edge case coverage
  - Impact: Reduced confidence in production behavior
  - Owner: QA team
  - ETA: T+10d

## LOW Priority

- [ ] **Documentation Sync**
  - Risk: Some handover docs reference old signatures
  - Impact: Developer confusion
  - Owner: Tech writers
  - ETA: T+14d

- [ ] **CI Audit Hooks**
  - Risk: No automated audit on policy changes
  - Impact: Manual verification required
  - Owner: DevOps team
  - ETA: T+14d

---

## Acceptance Criteria

### HIGH Priority
- [ ] All v1.1 rules trigger correctly in E2E test
- [ ] engine_summaries present in all Guard invocations
- [ ] Cross-engine consistency validated with 3 sample sets

### MEDIUM Priority
- [ ] conditions_met[] verified in 10 relation scenarios
- [ ] Test coverage ‚â• 2 cases per v1.1 rule
- [ ] Regression test suite passes

### LOW Priority
- [ ] All handover docs updated
- [ ] CI workflow runs on PR
- [ ] Audit reports auto-generated
"""

    write_report("stage2_gap_list.md", gap_list)

    # Action plan
    action_plan = """# Stage 2 ‚Äî Action Plan

## T+0~3d (Immediate)

### 1. Policy Signature Verification ‚úÖ
- **Owner:** DevOps
- **Tasks:**
  - [x] Run PSA verify on all policies
  - [ ] Re-sign any UNSIGNED policies
  - [ ] Update handover docs with new hashes
- **Acceptance:** All policies signed and verified

### 2. LLM Guard v1.1 Schema Validation ‚úÖ
- **Owner:** Backend
- **Tasks:**
  - [x] Validate v1.1 input/output schemas
  - [ ] Test with sample engine_summaries payload
  - [ ] Document required fields
- **Acceptance:** Schema validation passes with all required fields

## T+4~7d (Short-term)

### 3. engine_summaries Integration
- **Owner:** Analysis Service
- **Tasks:**
  - [ ] Identify injection point in analysis pipeline
  - [ ] Add engine_summaries builder
  - [ ] Unit test summaries construction
- **Acceptance:** engine_summaries present in all Guard calls

### 4. Cross-Engine Consistency E2E
- **Owner:** QA + Backend
- **Tasks:**
  - [ ] Create 3 test sets (Ïã†ÏïΩ/Ï§ëÌôî/Ïã†Í∞ï)
  - [ ] Run E2E with v1.1 Guard
  - [ ] Verify CONSIST-450 triggers correctly
- **Acceptance:** All 3 scenarios produce expected verdicts

### 5. Test Coverage Enhancement
- **Owner:** QA
- **Tasks:**
  - [ ] Add edge cases for v1.1 rules
  - [ ] Validate revise ‚Üí allow conversion rate
  - [ ] Document test case rationale
- **Acceptance:** ‚â•2 cases per v1.1 rule, ‚â•80% revise‚Üíallow rate

## T+8~14d (Medium-term)

### 6. Relation Weight conditions_met Propagation
- **Owner:** Relation Engine
- **Tasks:**
  - [ ] Verify conditions_met[] in Relation Weight output
  - [ ] Ensure propagation to Guard input
  - [ ] Test REL-OVERWEIGHT-410 triggering
- **Acceptance:** 10 test scenarios validated

### 7. Performance & Timeout Policies
- **Owner:** Backend
- **Tasks:**
  - [ ] Set Guard timeout ‚â§1500ms
  - [ ] Implement fallback on timeout
  - [ ] Add retry backoff for model errors
- **Acceptance:** p95 Guard latency <1.5s, 0 timeouts in 1000 calls

### 8. Documentation & Release
- **Owner:** Tech Writers + DevOps
- **Tasks:**
  - [ ] Sync all handover docs
  - [ ] Tag releases (llm-guard-v1.1.0, etc.)
  - [ ] Generate CHANGELOG entries
- **Acceptance:** All docs current, tags pushed

## T+15~21d (Long-term)

### 9. CI/CD Integration
- **Owner:** DevOps
- **Tasks:**
  - [ ] Add stage2_audit.yml to GitHub Actions
  - [ ] Auto-run on policy/** changes
  - [ ] Upload reports as artifacts
- **Acceptance:** CI passes on all PRs

### 10. Monitoring & Alerting
- **Owner:** SRE
- **Tasks:**
  - [ ] Add Guard metrics (verdict distribution, latency)
  - [ ] Set up alerts for high revise rate
  - [ ] Dashboard for rule trigger frequency
- **Acceptance:** Metrics visible in production

---

## Rollback Plan

If critical issues found:
1. Revert to v1.0 Guard (9 rules)
2. Disable cross-engine consistency checks
3. Investigate root cause offline
4. Re-deploy v1.1 after fix
"""

    write_report("stage2_action_plan.md", action_plan)

    # E2E smoke log
    e2e_log = """# E2E Smoke Test Log

**Status:** ‚è≥ PENDING (Runtime not available for automated execution)

---

## Test Scenarios

### Scenario 1: ALLOW (Low Risk)
**Input:**
- Strength: 0.32 (Ïã†ÏïΩ)
- Yongshin: ["Î™©"] (resource)
- Relation: sanhe=0.65 (Î™© Í∞ïÌôî)
- Climate: Î™© season, support=Í∞ïÌï®

**Expected:**
- Verdict: allow
- Risk Level: low
- Rules Fired: 0 violations

**Actual:** TBD

---

### Scenario 2: REVISE (Medium Risk)
**Input:**
- Strength: 0.52 (Ï§ëÌôî)
- Yongshin: ["Ìôî"]
- Relation: sanhe=0.92, conditions_met=["partial"], formed=false
- Climate: Ïàò season, support=ÏïΩÌï®

**Expected:**
- Verdict: revise
- Risk Level: medium
- Violations: REL-OVERWEIGHT-410, YONGSHIN-UNSUPPORTED-460

**Actual:** TBD

---

### Scenario 3: DENY (High Risk)
**Input:**
- Strength: 0.78 (Ïã†Í∞ï)
- Yongshin: ["Î™©"] (resource - inconsistent with Ïã†Í∞ï)
- Relation: chong=0.85, xing=0.70
- Climate: Í∏à season, support=Î≥¥ÌÜµ

**Expected:**
- Verdict: deny
- Risk Level: high
- Violations: CONSIST-450, REL-400

**Actual:** TBD

---

## Execution Instructions

To run E2E smoke tests:

1. Start analysis-service runtime
2. Prepare test payloads (see scenarios above)
3. Call Guard endpoint with each payload
4. Record verdicts, risk levels, violations
5. Verify against expected results

**Command:**
```bash
# TBD: Add when runtime available
pytest services/analysis-service/tests/test_llm_guard_e2e.py -v
```
"""

    write_report("e2e_smoke_log.md", e2e_log)

    print("[7/8] All reports generated")
    print("[8/8] Audit complete\n")

    print("=" * 60)
    print("Summary:")
    print(f"  ‚úÖ {ok} policies verified")
    print(f"  ‚ö†Ô∏è  {mis} policies need signing")
    print(f"  ‚ùå {err} policies have errors")
    print("  üìÅ 8 reports written to reports/")
    print("=" * 60)

    return 0


if __name__ == "__main__":
    sys.exit(main())
